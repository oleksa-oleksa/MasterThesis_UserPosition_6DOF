{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74ced15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "config_path = \"../UserPrediction6DOF/tools/config.toml\"\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbe1f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, test_ratio):\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def test_train_split(X, y, test_ratio):\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def add_sliding_window(X, y, seq_length, pred_step):\n",
    "    X_w = []\n",
    "    y_w = []\n",
    "\n",
    "    # SLIDING WINDOW LOOKING INTO PAST TO PREDICT 20 ROWS INTO FUTURE\n",
    "    for i in range(seq_length, len(X) - pred_step + 1):\n",
    "        X_w.append(X[i - seq_length:i, 0:X.shape[1]])\n",
    "        y_w.append(y[i:i + pred_step, 0:y.shape[1]])\n",
    "\n",
    "    X_w, y_w = np.array(X_w), np.array(y_w)\n",
    "\n",
    "    logging.info(\"------------- Creating 3D datasets and adding sliding window ------------\")\n",
    "    logging.info(f'X_w.shape: {X_w.shape}')\n",
    "    logging.info(f'y_w.shape: {y_w.shape}')\n",
    "    logging.info(f\"Sliding window of {seq_length} added and 3D datasets are created!\")\n",
    "    logging.info(\"--------\")\n",
    "    return X_w, y_w\n",
    "\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    logging.info(f\"---------------- Reading 2D dataset from {dataset_path} ----------------\")\n",
    "    df = pd.read_csv(os.path.join(dataset_path, \"dataset.csv\"))\n",
    "    logging.info(f\"Dataset shape: {df.shape}\")\n",
    "    logging.info(f'Columns: {list(df.columns)}')\n",
    "    logging.info(\"--------\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_X_y(df, features, seq_length, pred_step, outputs):\n",
    "    X = df[features].to_numpy()\n",
    "    logging.info(\"------------------ Creating 2D X and y datasets  -----------------------\")\n",
    "    logging.info(f'X.shape: {X.shape}')\n",
    "    logging.info(f'Using past {seq_length} values for predict in {pred_step} in future')\n",
    "\n",
    "    y = df[outputs].to_numpy()\n",
    "    logging.info(f'y.shape: {y.shape}')\n",
    "    logging.info('2D datasets X and y created')\n",
    "    logging.info(\"--------\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prepare_loaders(X_train, y_train, X_test, y_test, batch_size=64):\n",
    "    train_features = torch.Tensor(X_train)\n",
    "    train_targets = torch.Tensor(y_train)\n",
    "    test_features = torch.Tensor(X_test)\n",
    "    test_targets = torch.Tensor(y_test)\n",
    "\n",
    "    train = TensorDataset(train_features, train_targets)\n",
    "    test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_data(X_train, X_val, X_test, y_train, y_val, y_test, batch_size=64):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X_train:\n",
    "    :param X_val:\n",
    "    :param X_test:\n",
    "    :param y_train:\n",
    "    :param y_val:\n",
    "    :param y_test:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "\n",
    "    The drop_last=True parameter ignores the last batch\n",
    "    (when the number of examples in a dataset is not divisible\n",
    "    by a batch_size) while drop_last=False will make the last batch\n",
    "    smaller than a batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    train_features = torch.Tensor(X_train)\n",
    "    train_targets = torch.Tensor(y_train)\n",
    "    val_features = torch.Tensor(X_val)\n",
    "    val_targets = torch.Tensor(y_val)\n",
    "    test_features = torch.Tensor(X_test)\n",
    "    test_targets = torch.Tensor(y_test)\n",
    "\n",
    "    train = TensorDataset(train_features, train_targets)\n",
    "    val = TensorDataset(val_features, val_targets)\n",
    "    test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, test_loader_one\n",
    "\n",
    "\n",
    "def save_numpy_array(dataset_path, filename, np_array):\n",
    "    np.save(os.path.join(dataset_path, f'{filename}.npy'), np_array)\n",
    "    logging.info(f'WRITE: {filename} saved to {dataset_path}')\n",
    "\n",
    "\n",
    "def load_numpy_array(dataset_path, filename):\n",
    "    data = np.load(os.path.join(dataset_path, f'{filename}.npy'))\n",
    "    logging.info(f'READ: {filename} loaded from {dataset_path}')\n",
    "    logging.info(f'{filename}.shape: {data.shape}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44a6b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, features, outputs, dataset_path, results_path, pred_window=100):\n",
    "        self.cfg = toml.load(config_path)\n",
    "        self.dt = self.cfg['dt']\n",
    "        self.pred_window = pred_window * 1e-3  # convert to seconds\n",
    "        self.pred_step = int(self.pred_window / self.dt)\n",
    "        self.dataset_path = dataset_path\n",
    "        self.results_path = results_path\n",
    "        self.model = None  # set by select_model()\n",
    "        self.params = None  # set by select_model()\n",
    "        self.X, self.y = [], []\n",
    "        self.X_w, self.y_w = [], []\n",
    "        self.X_train, self.X_val, self.X_test = [], [], []\n",
    "        self.y_train, self.y_val, self.y_test = [], [], []\n",
    "        self.config = None\n",
    "        self.seq_length_input = 20  # input length of timeseries from the past\n",
    "        self.seq_length_output = self.pred_step  # output length of timeseries in the future\n",
    "        # -------------  FEATURES ---------------#\n",
    "        self.features = features\n",
    "        # only position and rotation\n",
    "        # self.features = self.cfg['pos_coords'] + self.cfg['quat_coords']\n",
    "\n",
    "        # --------------  OUTPUTS ---------------#\n",
    "        # position and rotation in future will be predicted\n",
    "        self.outputs = outputs\n",
    "    def _prepare_raw_dataset(self):\n",
    "        # Read full dataset from CSV file\n",
    "        df = load_dataset(self.dataset_path)\n",
    "        # create 2D arrays of features and outputs\n",
    "        self.X, self.y = prepare_X_y(df, self.features, self.seq_length_input, self.pred_step, self.outputs)\n",
    "\n",
    "    def _add_sliding_window(self):\n",
    "        # Features and outputs with sequence_len = sliding window\n",
    "        self.X_w, self.y_w = add_sliding_window(self.X, self.y, self.seq_length_input, self.pred_step)\n",
    "    \n",
    "    def _split(self):\n",
    "    # Splitting the data into train, validation, and test sets\n",
    "        self.X_train, self.X_val, self.X_test, \\\n",
    "            self.y_train, self.y_val, self.y_test = train_val_test_split(self.X_w, self.y_w, 0.2)\n",
    "\n",
    "        logging.info(f\"X_train {self.X_train.shape}, X_val {self.X_val.shape}, \"\n",
    "                     f\"X_test{self.X_test.shape}, y_train {self.y_train.shape}, \"\n",
    "                     f\"y_val {self.y_val.shape}, y_test {self.y_test.shape}\")\n",
    "\n",
    "        path = os.path.join(self.results_path, 'train_val_test')\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        save_numpy_array(path, 'X_train', self.X_train)\n",
    "        save_numpy_array(path, 'X_val', self.X_val)\n",
    "        save_numpy_array(path, 'X_test', self.X_test)\n",
    "        save_numpy_array(path, 'y_train', self.y_train)\n",
    "        save_numpy_array(path, 'y_val', self.y_val)\n",
    "        save_numpy_array(path, 'y_test', self.y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59c8d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toml.load(config_path)\n",
    "features_pos = cfg['pos_coords'] + cfg['velocity']\n",
    "features_pos_pure = cfg['pos_coords']\n",
    "outputs_pos = cfg['pos_coords']\n",
    "features_rot = cfg['quat_coords'] + cfg['velocity']\n",
    "features_rot_pure = cfg['quat_coords']\n",
    "outputs_rot = cfg['quat_coords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1f60930",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pos = Dataset(features_pos, outputs_pos, dataset_path='../data/flipped', results_path='../data/flipped/position')\n",
    "ds_pos._prepare_raw_dataset()\n",
    "ds_pos._add_sliding_window()\n",
    "ds_pos._split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35b3dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rot = Dataset(features_rot, outputs_rot, dataset_path='../data/flipped', results_path='../data/flipped/rotation')\n",
    "ds_rot._prepare_raw_dataset()\n",
    "ds_rot._add_sliding_window()\n",
    "ds_rot._split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "827b63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pos_pure = Dataset(features_pos_pure, outputs_pos, dataset_path='../data/flipped', results_path='../data/flipped/position_pure')\n",
    "ds_pos_pure._prepare_raw_dataset()\n",
    "ds_pos_pure._add_sliding_window()\n",
    "ds_pos_pure._split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c438e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rot_pure = Dataset(features_rot_pure, outputs_rot, dataset_path='../data/flipped', results_path='../data/flipped/rotation_pure')\n",
    "ds_rot_pure._prepare_raw_dataset()\n",
    "ds_rot_pure._add_sliding_window()\n",
    "ds_rot_pure._split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f404bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
