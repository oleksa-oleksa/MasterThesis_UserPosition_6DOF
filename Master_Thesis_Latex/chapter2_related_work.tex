% !TeX spellcheck = en
\section{Related works}
\label{sec:related}
This section presents the overview of previous research in the field of the prediction of user position and focuses on time series methods using different RNN architectures such as LSTM and GRU.  


\subsection{Traditional prediction algorithms}
\label{sec:related:timeseries}
A lot of previous approaches uses basic processing of head movement history to predict the future movement, such as simple average, linear regression, and weighted linear regression \cite{attention_saliency}. Work of \textit{Corbillon et al., 2017} determines the distance to the center of viewpoint with simple average and calculates the region that receives from server the video data with a better quality than the remaining of part the video \cite{simple_average}.\\ The work of \textit{Duanmu et al., 2017} proposes prediction of the viewing direction for segment n + 1 through linear regression based on the past view segments \cite{linreg1}. Approach of \textit{Xie et al., 2017} uses user’s orientation in Euler angle and leverage Linear Regression model to apply Least Square Method and to calculate the trends of head movements \cite{linreg2}. The work \cite{linreg3} proposes to receive from a server only the data of covered by user's viewport. At each point of time, the client requests data which would be played in the future. \textit{Taghavi et al., 2017} use Weighted Linear Regression to predict the next viewport based on window with the latest viewport samples. Researchers mentioned that a client can continue playback of at least a low-quality version of the video when the download time of next video portion varies \cite{linreg3}.\\
Analysis done by \textit{Qian et al., 2016} indicates that at least in the short term, viewers’ head movement can be predicted with accuracy > 90\% by even using simple methods such as linear regression \cite{cellular_opt}. The different approaches were compared such as computing the average value, using the linear regression with all samples and with weighted linear regression with recent samples. With weighted linear regression the average prediction accuracy for short-term values was higher than 90\% across all users. However in the longer term it is more difficult to achieve the good result and the average accuracy drops to about 70\% \cite{cellular_opt}.\\
A method to apply saliency algorithms to VR video viewings was presented by \textit{Aladagli et al., 2017} in work \cite{predicting_360}. Cross-correlation analysis used for measuring the relationship between the predicted fixation sequences and the recorded head movements \cite{predicting_360}. Based on works mentioned above, \textit{Nguyen et al., 2018} proposed panoramic saliency algorithm in order to learn the dependence of head tracking logs and saliency maps from the past video frames.\\

\subsection{Recurrent Neuronal Networks}
\label{sec:related:deep}
As was explained above, traditional prediction algorithms can not be used straightforward on a new media content in 6-DoF VR Applications. The user position and rotation data is coming as time series with a sequential order that is crucial to be followed in order to predict correctly the next future step for a look-ahead time. A sequence of inputs can be processed with Artificial Neural Network (ANN) called Recurrent Neural Network (RNN). Moreover, RNN can processes input with remembering its state while processing the next sequence of inputs. It is known that standard RNN has difficulties to learn long-term dependencies with gradient descent \cite{rnn_difficults}. Though RNN can robustly store information, it yields a problem of vanishing gradient that make leaning difficult \cite{rnn_difficults}. In the last decade, RNN algorithms have been adopted for motion prediction of 3D sequences with long-term dependencies taken into account. For example, the work of \textit{Crivellari et al., 2020} targets traces of tourists in a foreign country and tries to predict the motion of people in the environment they never seen before. LSTM-based model is used thus for analyzing the tourists’ mobility patterns \cite{tourist_traces}.\\
%================================================
The authors \textit{Aykut et al., 2018} claims their research to be first work that applies deep learning for head motion prediction. The authors experimentally confirmed that Feed-forward Neural Network (FFN) indeed had difficulties to learn for different delays. The decision to use LSTM-based architectures \textit{Aykut et al., 2018} reasoned with feedback loop and ability to establish a way of memory and share weights over time \cite{delay_compensation_360}. Conducted by researchers experiments showed that the LSTM-based architecture leads to a significant improvement of the MAE and RMSE metrics \cite{delay_compensation_360}. The LSTM-based methods were compared also to widely used approaches like the Linear Regression and a Kalman Filter based optimal state estimate. Thus \textit{Aykut et al., 2018} demonstrated a substantial improvement of the deep predictor for latencies in the range of 0.1–0.9 s \cite{delay_compensation_360}.\\
%================================================
Next year \textit{Aykut et al., 2019} experimented in their work \cite{telepresence} with GRU model that belongs to the group of recurrent neural networks (RNN). Authors considered GRU usage because it is computationally more efficient, as it has fewer parameters and states than LSTM units \cite{telepresence}. Proposed in the research GRU-based network is able to improve the MAE and RMSE compared to mentioned above LSTM model, especially for larger delays \cite{telepresence}. \\
%================================================
Researchers \textit{Karim et al., 2018} developed long short term memory fully convolutional network (LSTM-FCN). In the proposed models, LSTM block is augmented by an fully convolutional block \cite{lstm_fcn} identical to the convolution block in the CNN architecture proposed by \textit{Wang et al., 2018} in their work \cite{timeseries_scratch}. \textit{Karim et al., 2018} tried to reduce the rapid model's overfitting by transformation of input to have N variables with a single time step \cite{lstm_fcn}.\\
%================================================
In work of \textit{Chang et al., 2020} used in addition to standard LSTM networks also bidirectional LSTM (Bi-LSTM) networks, which is stacked two LSTM networks in forward and backward directions. Standard LSTM networks can only consider the past information and Bi-LSTM networks can capture both past and future information by two opposite temporal order in hidden layers \cite{6DoF_Tracking}. Experimentally, authors found that the basic LSTM performs the best comparing to Bi-LSTM and Temporal Convolutional Network  \cite{6DoF_Tracking}.\\
%================================================
The paper \cite{action_recognition} also aims for action recognition using sensor signals from HMD. Researches used GRU Model and additionally a bidirectional LSTM (Bi-LSTM) network.  Authors mentioned a representation of a signal data by the latent vector in the low-dimensional space after using the CAE model. The latent vector then will be clustered by K-Means Clustering so that centroids are referred to as motion bases in a model. For action tracking with LSTM, the displacement in each timestamp was predicted first and then added to its previous location, instead of predicting each position directly \cite{action_recognition}. Similar as in work \cite{6DoF_Tracking} the 3-layered LSTM model performed better compared to Bi-LSTM. Authors said that the possible reason could be the short-term correlation of human actions in their dataset and that Bi-LSTM with its complicated model structure is rather suitable for long-term actions \cite{action_recognition}.\\
%================================================
The paper of \textit{Chung et al., 2014} should be noted separately in the end of related works review because it provides an interesting compassion and evaluation of the performance of recurrent units LSTM and GRU on sequence modeling. Authors mentioned the ability of LSTM to keep the existing memory via the introduced gates and thus to detect an important feature from an input sequence at early stage, to easily carry this information (the existence of the feature) over a long distance, hence, capturing potential long-distance dependencies \cite{empirical_evaluation}. The GRU takes linear sum between the existing state and the newly computed state similar to the LSTM but does not have any mechanism to control the degree to which its state is exposed, but exposes the whole state each time \cite{empirical_evaluation}. \textit{Chung et al., 2014} emphasize the fact that any important feature, decided by either the forget gate of the LSTM unit or the update gate of the GRU, will not be overwritten but be maintained as it is \cite{empirical_evaluation}. LSTM unit controls the amount of the new memory content and does not have any separate control of the amount of information flowing from the previous time step. The GRU differs and controls the information flow from the previous activation when computing the new and does not independently control the amount of the candidate activation being added via update gate \cite{empirical_evaluation}. The experiments provided in this work clearly indicate the advantages of the gating units over the more traditional recurrent units. \textit{Chung et al., 2014} mentioned that with dataset they used GRU unit outperformed LSTM unit. But they suggest that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task \cite{empirical_evaluation}. Thus in section \ref{sec:design:dataset:explor} of chapter ``Data and Model'' the data exploration of the obtained from HMD Microsoft HoloLend is done in order to understand the dataset before the beginning with a development of model architecture. 