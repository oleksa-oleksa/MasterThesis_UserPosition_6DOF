% !TeX spellcheck = en
\chapter{Conclusion}
\label{sec:conclusion}
The different RNN-based architectures are designed in order to predict the future user's position and rotation in a 6-dimensional degree of freedom (6-DoF) of Extended Reality (XR) applications. The goal of evaluation of the designed models is an understanding whether proposed approaches can help to reduce the M2P latency of cloud-based streaming service for a given look-ahead time (LAT) of 100 ms. 

For a comparison of the results of the RNN-based models a simple Baseline model was implemented as a reference and the evaluation metrics were first calculated for this model. The experiments done in this thesis on a real head motion dataset collected from Microsoft HoloLens showed that proposed models can predict significant better than a Baseline model and thus it is reasonable to build the best model in the prediction engine of the cloud-based streaming service from work \cite{serhan_cloud_streaming}. It is worth mentioning the work of \textit{Serhan et al., 2020} where a Kalman filter-based framework for prediction of head motion was designed for the same cloud-based service \cite{serhan_kalman}. This master thesis presents the models that not only predict significantly better than Baseline model, but also have smaller $MAE$ and $RMSE$ metrics compared to a previous approach based on Kalman Filter. Thus the goal of the master thesis is reached and it can be clearly seen that GRU model can significantly reduce the M2P latency of network and computational delays.

First, the real 6-DoF dataset was obtained for the research and conducted experiments. For this purpose an application was developed in Unity with the Mixed Reality Toolkit (MRTK) and deployed on HoloLens 2. Main Camera in Unity is always the primary stereo rendering component attached to HMD and it is rendering everything the user sees and is automatically configured to track head movements. The settings allowed users to walk around within the 10-metre boundary. This is quite enough for the user's movements inside the laboratory space and simultaneously watching a volumetric animated object that is placed 3 metres ahead of the user in the VR environment. Using the Main Camera, 6-DoF datasets were logged in a csv-file.

The obtained from HoloLens 6-DoF dataset was first explored, preprocessed and finally discovered discontinuities in the quaternions were fixed and thus evaluation metrics can be improved compared to the original interpolated dataset. When LSTM architecture is predicted on an interpolated dataset, the average distance between predicted position and the real position is reduced from created by Baseline 7 cm to just 2 cm. But for all four quaternion components calculated metrics showed that LSTM1 predicts the rotation on an interpolated dataset worse than the Baseline model. The mean square error on fixed dataset with flipped negative reduced the distance between predicted position and the real position from 7 cm to approx. 1.3 cm For all four quaternion components the LSTM model predicts the rotation on a flipped dataset slightly better than the Baseline model compared to prediction with interpolated dataset the improvement is obvious. 

Summing up, it can be said that the LSTM-based architectures on flipped interpolated dataset leads to a significant improvement of the $MAE$ and $RMSE$ metrics compared to a Baseline model. One-layered LSTM improves $MAE_{pos}$ by 80\% and $MAE_{rot}$ by 7,5\% compared to a Baseline. The architecture supplemented with a linear layer $Mish$ activation function improves $MAE_{pos}$ by 82\% and $MAE_{rot}$ by 10\% compared to a Baseline once again.

The best performance, however, is achieved by the GRU-based model. The experiments proved that it is possible to improve $MAE_{pos}$ by 85\% and $MAE_{rot}$ by 40\% compared to a Baseline. Compared to LSTM prediction, GRU has smaller $MAE_{pos}$ by 24\% and $MAE_{rot}$ by 36\%. The average distance between predicted position and the real position is reduced from 7 cm and GRU1 average error is less than 1 cm. This is considered as an acceptable error for such a huge M2P latency of 100 ms. As it was said, selected for experiments M2P latency is higher than measured network and computational delays in cloud streaming service. Additionally, performing experiments for smaller latency resulted in even smaller $MAE$ and $RMSE$ metrics if compared to the metrics with 100 ms latency. It must be mentioned that a Baseline outputs for a smaller latency are literally the original outputs delayed for a smaller latency. As expected, the Baseline metrics are thus also decreased if M2P latency decreases. 

Any variant of layered architecture needs significantly more time to train and despite the fact of the complexer architecture can not catch the spatial dependencies in dataset and have higher training and validation errors. The bidirectional RNN-based model presented by Bi-GRU could not improve metrics of the unidirectional model. Bi-GRU performed approximately twice better for position prediction and even worse for rotation prediction than the Baseline.

The Python application $UserPrediction6DOF$ is a result of this work and can be used in the future for a preprocessing of the new obtained datasets, training routine and prediction of user position and rotation in 6-DoF virtual environment for different M2P latencies. 

\section{Analysis}
\label{sec:conclusion:analysis}
Same as in the work of \textit{Chang et al., 2020}, the basic GRU performs the best among all models. Work \cite{6DoF_Tracking} pointed out that it could be possibly due to the short-term correlation of human actions, so it isnâ€™t often required during prediction tasks to consider the long term complexity. The important feature of LSTM is an ability to keep the existing memory via the introduced gates and thus to detect an important feature from an input sequence at an early stage and carry this information (the existence of the feature) over a long distance. This results in ability to capture potential long-distance dependencies \cite{empirical_evaluation}. 

The GRU takes a linear sum between the existing state and the newly computed state similar to the LSTM but does not have any mechanism to control the degree to which its state is exposed, but exposes the whole state each time \cite{empirical_evaluation}. \textit{Chung et al., 2014} emphasise the fact that any important feature, decided by either the forget gate of the LSTM unit or the update gate of the GRU, will not be overwritten but maintained as it is \cite{empirical_evaluation}. The LSTM unit controls the amount of the new memory content and does not have any separate control of the amount of information flowing from the previous time step. The GRU differs and controls the information flow from the previous activation when computing the new and does not independently control the amount of the candidate activation being added via update gate \cite{empirical_evaluation}. 

For the prediction of rotational data the positional data can be eliminated from a dataset. The key to this phenomena can lay in the user behaviour during dataset recording. Because the users were told to keep their sight mainly on the volumetric object placed in the MR environment, users tended to turn their head always in direction to a VV even if they walked far enough from the object. Observations of users during data recording showed that users usually prefer to move away from the object, looking at it, and then turn around 180 degrees and continue moving towards the object back or go around it from one side. Thus position data and rotation data can be split and only rotational data and velocity can be used to predict the rotation of the user in XR Application.

In contrast to the above, the rotation must be kept in a dataset for prediction of the position. Eliminating the rotation data or/and the velocity does not improve the prediction error compared to a Baseline and to the prediction on the full interpolated and flipped dataset.

GRU1 results for rotation prediction on the full interpolated dataset with flipped negative quaternions outperformed the results of experiment done with a dataset containing only rotational data. 

The evaluation metrics for user's rotation predictions are ranged for $MAE_{rot}$ between $14.61^{\circ}$ for Baseline outputs and $8.68^{\circ}$ for best GPU predictions. At first glance, such a deviation from the real values may seem significant. The study of the human eyes field of vision showed\footcite{https://medium.com/@catalin.macovei/positioning-of-infotainment-screen-in-cars-e4cffa1e5697}  that humans have quite a wide angle of vision. Lateral field of view is 60 degrees in total, 30 degrees to left, 30 degrees to right without moving the eyes. Vertical field of view is 25 degrees upward and 15 degrees downward from the straight line of sight. If the user is looking straight ahead, the eyes should move less than 10 degrees down and maximum 30 degrees so that everything that happens in front of the user is still in field of view. Therefore, the result of this metric, despite the noticeable numerical value of the deviation in degrees, is considered as a successful result of evaluation, since the projected image created from the VV still remains in the human field of view.

The evaluated LSTM and GRU model both significantly improve the evaluation metrics. GRU model however performs the best and thus can be used for reducing the Motion-to-Photon (M2P) latency by predicting the future user position and orientation for a look-ahead time (LAT) and sending the corresponding rendered view to a client.

\section{Limitations and suggestions for future work}
\label{sec:conclusion:future}
Research in user's head motion prediction can be continued and advanced models implemented. It is recommended to continue experiments with the GRU-based model, since even simple GRU model has the smallest prediction error. Section \ref{sec:related:deep} mentions successful experiments in which the RNN model was combined with FFN. Additionally the multivariate time series input for FNN is passed through a dimension shuffle layer and \cite{lstm_fcn} claims to improve in such a way the efficiency of the model when the number of variables is less than the number of time steps. The implementation of an advanced GRU model combined with FNN is recommended as a next step in order to minimise the evaluation errors. 

As future work the investigations into modelling the rotation prediction with GRU-based model using only rotation and velocity data is recommended. The model will probably require the new hyperparameters to be found and different sequence length can be used compared to the model that predicts rotation with position. 

Dividing the prediction approach into two parallel flows in which one predicts position on full dataset and second rotation on rotational dataset could improve metrics separately for rotation and position. 

The next step would be accessing the eye gathering data from HoloLens via The Eye Tracking API and run experiments to see whether this additional data helps to reduce prediction error. It is worth mentioning that the developed Unity Application will require the user to grant app permission to use eye tracking information. If the user will reject permissions then the needed information for the model will be missed during HMD usage. 

Gathering the additional data using HoloLens Research Mode could give access to key sensors. It is important to understand that Research Mode is opened specifically for research applications that aren't intended for deployment. The information from grey-scale cameras, depth camera, accelerometer, gyro and even magnetometer can be accessed in Research Mode. Because of limitations such data only on research mode these additional sensors were not read during the data collection for this master thesis. 

At the end of the thesis, it is worth pointing out that the proposed models will be beneficial in user's head motion prediction, and can quickly be deployed in real-time systems and embedded systems because the proposed models are small and efficient.