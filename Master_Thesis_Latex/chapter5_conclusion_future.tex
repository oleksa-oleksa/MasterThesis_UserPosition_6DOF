% !TeX spellcheck = en
\chapter{Conclusion}
\label{sec:conclusion}
The different RNN-based architectures are designed in order to predict the future user's position and rotation in a 6-dimensional degree of freedom (6-DoF) of Extended Reality (XR) applications. The goal of evaluation of the designed models is an understanding whether proposed approaches can help to reduce the M2P latency of cloud-based streaming service for a given look-ahead time (LAT) of 100 ms. 

For a compassion of the results of the RNN-based models a simple Baseline model was implemented as a reference and the evaluation metrics were first calculated for this model. The experiments done in this thesis on a real head motion dataset collected from Microsoft HoloLens showed that proposed models can predict significant better than a Baseline model and thus it is reasonable to build the best model in the prediction engine of the cloud-based streaming service from work \cite{serhan_cloud_streaming}. It is worth to mention the work of \textit{Serhan et al., 2020} where a Kalman filter-based framework for prediction of head motion was designed for the same cloud-based service \cite{serhan_kalman}. This master thesis presents the models that not only predict significant better than Baseline model, but also have smaller $MAE$ and $RMSE$ metrics comparing to a previous approach based on Kalman Filter. Thus the goal of the master thesis is reached and it can be clear seen that GRU model can significant reduce the M2P latency of network and computational delays.  

First, the real 6-DoF dataset was obtained for the research and conducted experiments.For this purpose an application was developed in Unity with the Mixed Reality Toolkit (MRTK) and deployed on HoloLens 2. Main Camera in Unity is always the primary stereo rendering component attached to HMD and it is rendering everything the user sees and is automatically configured to track head movements. The settings allowed users walk around within the 10-meter boundary. This is quite enough for user’s movements inside the laboratory space and simultaneously watching a volumetric animated object that is placed 3 metres ahead of the user in the VR environment. Using the Main Camera, 6-DoF datasets were logged in a csv-file.

The obtained from HoloLens 6-DoF dataset was first explored, preprocessed and finally discovered discontinuities in the quaternions were fixed and thus evaluation metrics can be improved compared to the original interpolated dataset. When LSTM architecture predicted on interpolated dataset, the average distance between predicted position and the real position reduced from created by Baseline 7 cm to just 2 cm. But for all four quaternion components calculated metrics showed that LSTM1 predicts the rotation on an interpolated dataset worse than the Baseline model. The mean square error on fixed dataset with flipped negative reduced the distance between predicted position and the real position from 7 cm to approx. 1.3 cm For all four quaternion components LSTM model predicts the rotation on an flipped dataset slightly better than the Baseline model compared to prediction with interpolated dataset the improvement is obvious. 

Summing up, it can be said that the LSTM-based architectures on flipped interpolated dataset leads to a significant improvement of the $MAE$ and $RMSE$ metrics compared to a Baseline model. One-layered LSTM improves $MAE_{pos}$ by 80\% and  $MAE_{rot}$ by 7,5\% compared to a Baseline. The architecture supplemented with linear layer $Mish$ activation function improves $MAE_{pos}$ by 82\% and  $MAE_{rot}$ by 10\% compared to a Baseline once again.

The best performance, however, is achieved by the GRU-based model. The experiments proved that it is possible to improve $MAE_{pos}$ by 85\% and  $MAE_{rot}$ by 40\% compared to a Baseline. Compared to LSTM prediction, GRU has smaller $MAE_{pos}$ by 24\% and  $MAE_{rot}$ by 36\%. The average distance between predicted position and the real position is reduced from 7 cm and GRU1 average error is less than 1 cm. This is considered as a acceptable error for a such huge M2P latency of 100 ms. As it was said,selected for experiments M2P latency is higher than measured network and computational delays in cloud streaming service. Additionally performed experiments for smaller latency resulted in even smaller $MAE$ and $RMSE$ metrics if compared to the metrics with 100 ms latency. Is must be mentioned that a Baseline outputs for a smaller latency are literally the original outputs delayed for a smaller latency. As expected, the Baseline metrics are thus also decreased if M2P latency decreases. 

Any variant of layered architecture needs significant more time to train and neglecting the fact of the complexer architecture can not catch the spatial dependencies in dataset and have higher training and validation errors. The bidirectional RNN-based model presented by Bi-GRU could not improve metrics of the unidirectional model. Bi-GRU performed approximately twice better for position prediction and even worse for rotation prediction than the Baseline.  

The Python application $UserPrediction6DOF$ is a result of this work and can be used in the future for a preprocessing of the new obtained datasets, training routine and prediction of user position and rotation in 6-DoF virtual environment for different M2P latencies. 

\section{Analysis}
\label{sec:conclusion:analysis}
Same as in work of \textit{Chang et al., 2020}, the basic GRU performs the best among all models. Work \cite{6DoF_Tracking} pointed that it could be possibly due to the short-term correlation of human actions, so it isn’t often required during prediction task to consider the longterm complexity. The important feature of LSTM is an ability to keep the existing memory via the introduced gates and thus to detect an important feature from an input sequence at early stage and carry this information (the existence of the feature) over a long distance. This results in ability to capture potential long-distance dependencies \cite{empirical_evaluation}. 

The GRU takes linear sum between the existing state and the newly computed state similar to the LSTM but does not have any mechanism to control the degree to which its state is exposed, but exposes the whole state each time \cite{empirical_evaluation}. \textit{Chung et al., 2014} emphasize the fact that any important feature, decided by either the forget gate of the LSTM unit or the update gate of the GRU, will not be overwritten but maintained as it is \cite{empirical_evaluation}. LSTM unit controls the amount of the new memory content and does not have any separate control of the amount of information flowing from the previous time step. The GRU differs and controls the information flow from the previous activation when computing the new and does not independently control the amount of the candidate activation being added via update gate \cite{empirical_evaluation}. 

For the prediction of rotational data the positional data can be eliminated from a dataset. The key to this phenomena can lay in the user behavior during dataset recording. Because the users were told to keep their sight mainly on the volumetric object placed in VR environment, users tended to turn their head always in direction to a VV even if they walked far enough from the object. Observations of users during data recording showed that users usually prefer to move away from the object, looking at it, and then turn around 180 degrees and continue moving towards the object back or go around it from one side. Thus position data and rotation data can be split and only rotational data and velocity can be used to predict the rotation of the user in VR Application.

In contrast to the above, the rotation must be kept in a dataset for prediction the position. Elimination the rotation data or/and the velocity does not improve the prediction error compared to a Baseline and to the prediction on the full interpolated and flipped dataset.  

GRU1 results for rotation prediction on the full interpolated dataset with flipped negative quaternions outperformed the results of experiment done with a dataset containing only rotational data. 

The evaluated LSTM and GRU model both significant improve the evaluation metrics. GRU model however performs the best and thus can be used for a reducing the Motion-to-Photon (M2P) latency by predicting the future user position and orientation for a look-ahead time (LAT) and sending the corresponding rendered view to a client. 

\section{Suggestions for future work}
\label{sec:conclusion:future}

Only rotation with GRU, new parameters for rotation
combining the gru with FFN
eye gathering data from hololens
research mode and data from accelerometer and gyro


